---
layout: post
title: "Day 32 â€“ Comparing Activation Functions"
date: 2025-07-09
author: Michael Orishagbemi
permalink: /day32.html
tags: ['Machine Learning', 'Alzheimers Disease', 'Sigmoid', 'Tanh', 'ReLU']

what_i_learned: |

  Today was another day of fulfilling the methodology portion of my research paper by comparing the evaluation metrics (Accuracy, MSE, MAE, etc) of my custom ELM to other ML models. Today I was focused on comparing the activation functions within my model: Sigmoid, Tanh, & ReLU. Admittedly I still don't have the best conceptual understanding of these functions but from I can summarize is that within a neural network (ELM) these functions introduce different ways for the network learn complex patterns by introducing non-linearty in the hidden layer of the network. In my work, I compared the same version of my ELM with different activation functions using a balanced dataset and cross-validation. Afterwards, I did the same thing with unbalanced data and balanced data with the train-test-split method.

blockers: |
  No blockers. It took a while though.
  
reflection: |

  Today was pretty similar to yesterday as the whole process with cross-validation isn't one I particularly find interesting. Thanks to yesterday's work I didn't have to start the cross-validation process all over again but I still had to make some edits to accomdate the new ELM's I was introducing. At first I had trouble implementing the activation functions because I didn't have a frame of reference of how to do it, I assumed it was similar to the parameter but its actually something you have to set up inside the ELM. My solution was to ask Ms. Amara for help then with code structure she provided for the activations, I made a copy of the ELM and pasted them inside. Allowing me to finally run the evaluations.
---


