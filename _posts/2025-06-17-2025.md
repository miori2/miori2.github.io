---
layout: post
title: "Day 16 â€“ Support Vector Machine & Random Forest Machine"
date: 2025-06-17
author: Michael Orishagbemi
permalink: /day16.html
tags: ["Machine Learning", "SVM", "Decision Trees", "Random Forest", "One-Hot Encoding"]

what_i_learned: |
  Today I learned more Machine Learning methods. I first learned about the classification algorithm Support Vector Machine which like the previous ones we went over, is an algorithm fit for regression and classification tasks and is good at handling linear and nonlinear data. We went through a similar process as ELM: splitting the dataset into test and training sets and scaling our data to avoid any optimization problems. The main difference is that we learn how to do SVM visualizations by plottting our data after predictions. Basically when plot the data you can choose your kernel to give you a model based off that data which changes the accuracy percentage of the model. I also learned about the random forest algorithm which is essentially a collection of decision trees where the goal is to have each tree as diverse as possible.
 
blockers: |
  No blockerrs today.
  
reflection: |
  Maybe it was due to the videos being more hands-on then the previous ones, but I felt like the implementations of SVM and Random Forest are much easier to grasp than that of ELM and KNN. I feel like those videos we helped me out with the whole splitting of the datasets into training and testing data. I'm also really enjoy the plotting aspect of SVM, each kernel has it own customizable parameters which can greatly affect the accuracy, I had a different result from the video even though I did the same things they did, which I find interesting. I'm a little less confident in my knowledge the Random Forest machine but at the very least I understand the mechanisms of RF (Bagging, Feature Randomization and Variance).
---
