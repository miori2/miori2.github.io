---
layout: post
title: "Day 30 â€“ Cross-Validation on Datasets"
date: 2025-07-07
author: Michael Orishagbemi
permalink: /day30.html
tags: ['Machine Learning', 'Alzheimers Disease', 'Cross-Validation', 'ELM']

what_i_learned: |
  Today I finished implementing the custom ELM our graduate mentor designed for us in my research paper. I was able to get the evaluation metrics for both balanced and unbalanced data. I then trained my ML models on the newly balanced data and got the metrics for them. Afterwards I intialized visualizations of my data in different forms. I created a histogram for my data comparing the testing and training RMSE scores of the ML methods I employed and built a heatmap for the whole data set to view my clusters. Finally I began k-fold cross-validation process on my unbalanced data where I choose how many folds I want my data to be divided into with the model being trained on all folds except for the latest one: k-1.

blockers: |
  No blockers!
  
reflection: |
  Today was I was going through the work I've done and making sure everything was done correctly. At the beginning of the day I restarted my kernel and checked if I had made any mistakes when cleaning my data and splitting it, I also made sure the results of some of my models were replicable with the random_state parameter. I had to look back at my previous work when it came to plotting graphs. The hardest part of me today was implementing cross-validation, my only reference to the technique was a research paper Ms. Amara assigned to us and the source code provided was difficult to understand. I was finally able to use it to get the evaluation metrics for my custom ELM.
---

